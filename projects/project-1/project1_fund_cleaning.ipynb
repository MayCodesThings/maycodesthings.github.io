{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee7ac8f",
   "metadata": {},
   "source": [
    "# Project 1: Fund Name Cleaning & Matching\n",
    "\n",
    "This Jupyter notebook contains Python scripts and logic used to clean, normalize, and group similar fund names for a real-world data cleaning task. The notebook consolidates multiple tasks, such as:\n",
    "\n",
    "- Removing legal suffixes\n",
    "- Normalizing numbers (words and Roman numerals)\n",
    "- Identifying exact and pattern-based fund name matches\n",
    "- Grouping funds by cleaned names and validating fund manager consistency\n",
    "\n",
    "Each section is labeled by task for clarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bce9deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/may/Downloads/Project _1/funds16052025.csv\") \n",
    "duplicates = df[df.duplicated(subset=['fund_name'], keep=False)]\n",
    "duplicates.to_csv('fund_name_duplicates.csv', index=False)\n",
    "print(\"fund_name_duplicates.csv\")\n",
    "\n",
    "\n",
    "# Task 2\n",
    "# Remove the list of legal suffixes from original fund file \n",
    "# Print the cleansed names of the funds\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "with open(\"/Users/may/Downloads/Project _1/fundExc:Users:may:Downloads:Project _1:fundExclusions.jsonlusions.json\", \"r\") as f:\n",
    "    suffixes = json.load(f)\n",
    "\n",
    "suffixes = [re.sub(r'[^a-zA-Z0-9]', '', s.lower()) for s in suffixes]\n",
    "\n",
    "number_words = {\n",
    "    \"first\": \"1\", \"second\": \"2\", \"third\": \"3\", \"fourth\": \"4\", \"fifth\": \"5\", \"sixth\": \"6\", \"seventh\": \"7\",\n",
    "    \"eighth\": \"8\", \"ninth\": \"9\", \"tenth\": \"10\", \"eleventh\": \"11\", \"twelfth\": \"12\",\n",
    "    \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\", \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\",\n",
    "    \"eight\": \"8\", \"nine\": \"9\", \"ten\": \"10\", \"eleven\": \"11\", \"twelve\": \"12\"\n",
    "}\n",
    "roman_numerals = {\n",
    "    \"i\": \"1\", \"ii\": \"2\", \"iii\": \"3\", \"iv\": \"4\", \"v\": \"5\", \"vi\": \"6\", \"vii\": \"7\",\n",
    "    \"viii\": \"8\", \"ix\": \"9\", \"x\": \"10\", \"xi\": \"11\", \"xii\": \"12\"\n",
    "}\n",
    "\n",
    "df = pd.read_csv(\"/Users/may/Downloads/Project _1/funds16052025.csv\")\n",
    "\n",
    "def remove_legal_suffix(name):\n",
    "    original_name = str(name).strip()\n",
    "    normalized = re.sub(r'[^a-zA-Z0-9 ]', '', original_name.lower()).strip()\n",
    "    for suffix in suffixes:\n",
    "        if normalized.endswith(suffix):\n",
    "            pattern = rf'[\\s,\\.]*{re.escape(suffix)}[\\s,\\.]*$'\n",
    "            cleaned = re.sub(pattern, '', original_name, flags=re.IGNORECASE).strip()\n",
    "            return cleaned, suffix.upper()\n",
    "    return original_name, \"-\"\n",
    "\n",
    "def normalize_numbers(name):\n",
    "    words = name.split()\n",
    "    result = []\n",
    "    changes = []\n",
    "    for word in words:\n",
    "        word_lower = word.lower()\n",
    "        clean_word = re.sub(r'[^a-zA-Z]', '', word_lower)\n",
    "        if clean_word in number_words:\n",
    "            result.append(number_words[clean_word])\n",
    "            changes.append(f\"{word} → {number_words[clean_word]}\")\n",
    "        elif clean_word in roman_numerals:\n",
    "            result.append(roman_numerals[clean_word])\n",
    "            changes.append(f\"{word} → {roman_numerals[clean_word]}\")\n",
    "        else:\n",
    "            result.append(word)\n",
    "    return \" \".join(result), \", \".join(changes) if changes else \"-\"\n",
    "\n",
    "cleaned_names = []\n",
    "removed_suffixes = []\n",
    "number_changes = []\n",
    "\n",
    "for name in df[\"fund_name\"]:\n",
    "    no_suffix_name, suffix_removed = remove_legal_suffix(name)\n",
    "    final_name, number_change = normalize_numbers(no_suffix_name)\n",
    "    cleaned_names.append(final_name)\n",
    "    removed_suffixes.append(suffix_removed)\n",
    "    number_changes.append(number_change)\n",
    "\n",
    "df[\"Cleaned Fund Name\"] = cleaned_names\n",
    "df[\"Removed Suffix\"] = removed_suffixes\n",
    "df[\"Number Normalization\"] = number_changes\n",
    "\n",
    "df.to_csv(\"cleaned_fund_names_with_details.csv\", index=False)\n",
    "\n",
    "with open(\"cleaned_fund_names_list.txt\", \"w\") as f:\n",
    "    for name in df[\"Cleaned Fund Name\"]:\n",
    "        f.write(name + \"\\n\")\n",
    "\n",
    "duplicates_df = df[df.duplicated(subset=[\"Cleaned Fund Name\"], keep=False)].copy()\n",
    "duplicates_df.to_csv(\"duplicate_cleaned_fund_names.csv\", index=False)\n",
    "\n",
    "duplicates_grouped = (\n",
    "    df[df.duplicated(subset=[\"Cleaned Fund Name\"], keep=False)]\n",
    "    .groupby(\"Cleaned Fund Name\")[\"fund_name\"]\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "matched_ids = (\n",
    "    df[df[\"Cleaned Fund Name\"].isin(duplicates_grouped[\"Cleaned Fund Name\"])]\n",
    "    .groupby(\"Cleaned Fund Name\")[\"fund_manager_id\"]\n",
    "    .apply(lambda x: sorted(set(x.dropna().astype(str))))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "merged = pd.merge(duplicates_grouped, matched_ids, on=\"Cleaned Fund Name\")\n",
    "\n",
    "expanded_funds = merged[\"fund_name\"].apply(pd.Series)\n",
    "expanded_funds.columns = [f\"Fund {i+1}\" for i in expanded_funds.columns]\n",
    "\n",
    "expanded_funds[\"Fund Manager IDs\"] = merged[\"fund_manager_id\"].apply(lambda x: \", \".join(x))\n",
    "expanded_funds.to_csv(\"exact_fund_name_matches.csv\", index=False)\n",
    "\n",
    "print(\"exact_fund_name_matches.csv' with Fund 1, Fund 2, ..., Fund Manager IDs\")\n",
    "\n",
    "# Task3\n",
    "# Normalise the numbers in the fund names to always be numerical rather than words or roman numerals \n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "with open(\"/Users/may/Downloads/Project _1/fundExc:Users:may:Downloads:Project _1:fundExclusions.jsonlusions.json\", \"r\") as f:\n",
    "    suffixes = json.load(f)\n",
    "\n",
    "suffixes = [re.sub(r'[^a-zA-Z0-9]', '', s.lower()) for s in suffixes]\n",
    "\n",
    "number_words = {\n",
    "    \"first\": \"1\", \"second\": \"2\", \"third\": \"3\", \"fourth\": \"4\", \"fifth\": \"5\", \"sixth\": \"6\", \"seventh\": \"7\",\n",
    "    \"eighth\": \"8\", \"ninth\": \"9\", \"tenth\": \"10\", \"eleventh\": \"11\", \"twelfth\": \"12\",\n",
    "    \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\", \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\",\n",
    "    \"eight\": \"8\", \"nine\": \"9\", \"ten\": \"10\", \"eleven\": \"11\", \"twelve\": \"12\"\n",
    "}\n",
    "\n",
    "roman_numerals = {\n",
    "    \"i\": \"1\", \"ii\": \"2\", \"iii\": \"3\", \"iv\": \"4\", \"v\": \"5\", \"vi\": \"6\", \"vii\": \"7\",\n",
    "    \"viii\": \"8\", \"ix\": \"9\", \"x\": \"10\", \"xi\": \"11\", \"xii\": \"12\"\n",
    "}\n",
    "\n",
    "df = pd.read_csv(\"/Users/may/Downloads/Project _1/funds16052025.csv\")\n",
    "\n",
    "def remove_legal_suffix(name):\n",
    "    original_name = str(name).strip()\n",
    "    normalized = re.sub(r'[^a-zA-Z0-9 ]', '', original_name.lower()).strip()\n",
    "\n",
    "    for suffix in suffixes:\n",
    "        if normalized.endswith(suffix):\n",
    "            pattern = rf'[\\s,\\.]*{re.escape(suffix)}[\\s,\\.]*$'\n",
    "            cleaned = re.sub(pattern, '', original_name, flags=re.IGNORECASE).strip()\n",
    "            return cleaned, suffix.upper()  \n",
    "    return original_name, \"-\"  \n",
    "\n",
    "def normalize_numbers(name):\n",
    "    words = name.split()\n",
    "    result = []\n",
    "    changes = []\n",
    "\n",
    "    for word in words:\n",
    "        word_lower = word.lower()\n",
    "        clean_word = re.sub(r'[^a-zA-Z]', '', word_lower)\n",
    "\n",
    "        if clean_word in number_words:\n",
    "            result.append(number_words[clean_word])\n",
    "            changes.append(f\"{word} → {number_words[clean_word]}\")\n",
    "        elif clean_word in roman_numerals:\n",
    "            result.append(roman_numerals[clean_word])\n",
    "            changes.append(f\"{word} → {roman_numerals[clean_word]}\")\n",
    "        else:\n",
    "            result.append(word)\n",
    "\n",
    "    return \" \".join(result), \", \".join(changes) if changes else \"-\"\n",
    "\n",
    "cleaned_names = []\n",
    "removed_suffixes = []\n",
    "number_changes = []\n",
    "\n",
    "for name in df[\"fund_name\"]:\n",
    "    no_suffix_name, suffix_removed = remove_legal_suffix(name)\n",
    "    final_name, number_change = normalize_numbers(no_suffix_name)\n",
    "\n",
    "    cleaned_names.append(final_name)\n",
    "    removed_suffixes.append(suffix_removed)\n",
    "    number_changes.append(number_change)\n",
    "\n",
    "df[\"Cleaned Fund Name\"] = cleaned_names\n",
    "df[\"Removed Suffix\"] = removed_suffixes\n",
    "df[\"Number Normalization\"] = number_changes\n",
    "\n",
    "df.to_csv(\"cleaned_fund_names_with_details.csv\", index=False)\n",
    "\n",
    "with open(\"cleaned_fund_names_list.txt\", \"w\") as f:\n",
    "    for name in cleaned_names:\n",
    "        f.write(name + \"\\n\")\n",
    "\n",
    "print(\"\\n📋 Final List of Cleaned Fund Names:\\n\")\n",
    "for name in cleaned_names:\n",
    "    print(name)\n",
    "\n",
    "print(\"\\n✅ Files saved:\")\n",
    "print(\"  → cleaned_fund_names_with_details.csv\")\n",
    "print(\"  → cleaned_fund_names_list.txt\")\n",
    "\n",
    "# Task 4: \n",
    "# Combine all fund together ( remove legal suffixes, normalise the numbers and look for more exact matches )\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "with open(\"/Users/may/Downloads/Project _1/fundExc:Users:may:Downloads:Project _1:fundExclusions.jsonlusions.json\", \"r\") as f:\n",
    "    suffixes = json.load(f)\n",
    "\n",
    "suffixes = [re.sub(r'[^a-zA-Z0-9]', '', s.lower()) for s in suffixes]\n",
    "\n",
    "number_words = {\n",
    "    \"first\": \"1\", \"second\": \"2\", \"third\": \"3\", \"fourth\": \"4\", \"fifth\": \"5\", \"sixth\": \"6\", \"seventh\": \"7\",\n",
    "    \"eighth\": \"8\", \"ninth\": \"9\", \"tenth\": \"10\", \"eleventh\": \"11\", \"twelfth\": \"12\",\n",
    "    \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\", \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\",\n",
    "    \"eight\": \"8\", \"nine\": \"9\", \"ten\": \"10\", \"eleven\": \"11\", \"twelve\": \"12\"\n",
    "}\n",
    "\n",
    "roman_numerals = {\n",
    "    \"i\": \"1\", \"ii\": \"2\", \"iii\": \"3\", \"iv\": \"4\", \"v\": \"5\", \"vi\": \"6\", \"vii\": \"7\",\n",
    "    \"viii\": \"8\", \"ix\": \"9\", \"x\": \"10\", \"xi\": \"11\", \"xii\": \"12\"\n",
    "}\n",
    "\n",
    "df = pd.read_csv(\"/Users/may/Downloads/Project _1/funds16052025.csv\")\n",
    "\n",
    "def remove_legal_suffix(name):\n",
    "    original_name = str(name).strip()\n",
    "    normalized = re.sub(r'[^a-zA-Z0-9 ]', '', original_name.lower()).strip()\n",
    "\n",
    "    for suffix in suffixes:\n",
    "        if normalized.endswith(suffix):\n",
    "            pattern = rf'[\\s,\\.]*{re.escape(suffix)}[\\s,\\.]*$'\n",
    "            cleaned = re.sub(pattern, '', original_name, flags=re.IGNORECASE).strip()\n",
    "            return cleaned, suffix.upper()\n",
    "    return original_name, \"-\"\n",
    "\n",
    "def normalize_numbers(name):\n",
    "    words = name.split()\n",
    "    result = []\n",
    "    changes = []\n",
    "\n",
    "    for word in words:\n",
    "        word_lower = word.lower()\n",
    "        clean_word = re.sub(r'[^a-zA-Z]', '', word_lower)\n",
    "\n",
    "        if clean_word in number_words:\n",
    "            result.append(number_words[clean_word])\n",
    "            changes.append(f\"{word} → {number_words[clean_word]}\")\n",
    "        elif clean_word in roman_numerals:\n",
    "            result.append(roman_numerals[clean_word])\n",
    "            changes.append(f\"{word} → {roman_numerals[clean_word]}\")\n",
    "        else:\n",
    "            result.append(word)\n",
    "\n",
    "    return \" \".join(result), \", \".join(changes) if changes else \"-\"\n",
    "\n",
    "cleaned_names = []\n",
    "removed_suffixes = []\n",
    "number_changes = []\n",
    "\n",
    "for name in df[\"fund_name\"]:\n",
    "    no_suffix_name, suffix_removed = remove_legal_suffix(name)\n",
    "    final_name, number_change = normalize_numbers(no_suffix_name)\n",
    "\n",
    "    cleaned_names.append(final_name)\n",
    "    removed_suffixes.append(suffix_removed)\n",
    "    number_changes.append(number_change)\n",
    "\n",
    "df[\"Cleaned Fund Name\"] = cleaned_names\n",
    "df[\"Removed Suffix\"] = removed_suffixes\n",
    "df[\"Number Normalization\"] = number_changes\n",
    "\n",
    "df.to_csv(\"cleaned_fund_names_with_details.csv\", index=False)\n",
    "\n",
    "with open(\"cleaned_fund_names_list.txt\", \"w\") as f:\n",
    "    for name in df[\"Cleaned Fund Name\"]:\n",
    "        f.write(name + \"\\n\")\n",
    "\n",
    "duplicates_df = df[df.duplicated(subset=[\"Cleaned Fund Name\"], keep=False)].copy()\n",
    "duplicates_df.to_csv(\"duplicate_cleaned_fund_names.csv\", index=False)\n",
    "\n",
    "print(\"\\n📋 Final List of Cleaned Fund Names:\\n\")\n",
    "for name in df[\"Cleaned Fund Name\"]:\n",
    "    print(name)\n",
    "\n",
    "print(\"\\n📝 Duplicates:\")\n",
    "for name in duplicates_df[\"Cleaned Fund Name\"].unique():\n",
    "    print(f\"→ {name}\")\n",
    "\n",
    "duplicates_grouped = (\n",
    "    df[df.duplicated(subset=[\"Cleaned Fund Name\"], keep=False)]\n",
    "    .groupby(\"Cleaned Fund Name\")[\"fund_name\"]\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "expanded_rows = duplicates_grouped[\"fund_name\"].apply(pd.Series)\n",
    "expanded_rows.columns = [f\"Match {i+1}\" for i in expanded_rows.columns]\n",
    "expanded_rows.to_csv(\"exact_fund_name_matches.csv\", index=False)\n",
    "\n",
    "print(\"\\n📁 Grouped matches saved to 'exact_fund_name_matches.csv'\")\n",
    "\n",
    "# Task 5\n",
    "# Remove all matched funds from the original fund file and output a new CSV containing only the unmatched funds.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_original = pd.read_csv(\"/Users/may/Downloads/Project _1/funds16052025.csv\")\n",
    "df_matches = pd.read_csv(\"exact_fund_name_matches.csv\")\n",
    "\n",
    "matched_funds = pd.unique(df_matches.values.ravel())\n",
    "matched_funds = [str(name).strip() for name in matched_funds if pd.notna(name)]\n",
    "\n",
    "filtered_df = df_original[~df_original[\"fund_name\"].isin(matched_funds)]\n",
    "filtered_df.to_csv(\"funds_without_matches.csv\", index=False)\n",
    "\n",
    "print(\"funds_without_matches.csv' (excluding matched funds)\")\n",
    "\n",
    "\n",
    "# Task 6\n",
    "# Identifying Subtle Fund Name Matches for Manual Review\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df_original = pd.read_csv(\"/Users/may/Downloads/Project _1/funds16052025.csv\")\n",
    "df_matches = pd.read_csv(\"exact_fund_name_matches.csv\")\n",
    "\n",
    "matched_funds = pd.unique(df_matches.values.ravel())\n",
    "matched_funds = [str(name).strip() for name in matched_funds if pd.notna(name)]\n",
    "\n",
    "filtered_df = df_original[~df_original[\"fund_name\"].isin(matched_funds)]\n",
    "\n",
    "ignore_words = {\"venture\", \"ventures\", \"fund\"}\n",
    "\n",
    "num_map = {\n",
    "    \"first\": \"1\", \"second\": \"2\", \"third\": \"3\", \"fourth\": \"4\", \"fifth\": \"5\", \"sixth\": \"6\",\n",
    "    \"seventh\": \"7\", \"eighth\": \"8\", \"ninth\": \"9\", \"tenth\": \"10\", \"eleventh\": \"11\", \"twelfth\": \"12\",\n",
    "    \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\", \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\",\n",
    "    \"eight\": \"8\", \"nine\": \"9\", \"ten\": \"10\", \"eleven\": \"11\", \"twelve\": \"12\",\n",
    "    \"i\": \"1\", \"ii\": \"2\", \"iii\": \"3\", \"iv\": \"4\", \"v\": \"5\", \"vi\": \"6\", \"vii\": \"7\",\n",
    "    \"viii\": \"8\", \"ix\": \"9\", \"x\": \"10\", \"xi\": \"11\", \"xii\": \"12\"\n",
    "}\n",
    "\n",
    "def standardize(name):\n",
    "    words = re.sub(r'[^a-zA-Z0-9 ]', '', str(name).lower()).split()\n",
    "    clean = []\n",
    "    for word in words:\n",
    "        w = re.sub(r'[^a-zA-Z0-9]', '', word)\n",
    "        if w in num_map:\n",
    "            clean.append(num_map[w])\n",
    "        elif w not in ignore_words:\n",
    "            clean.append(w)\n",
    "    clean = [w for w in clean if w != \"1\"] \n",
    "    return \" \".join(sorted(clean))\n",
    "\n",
    "filtered_df[\"pattern_group\"] = filtered_df[\"fund_name\"].apply(standardize)\n",
    "\n",
    "grouped = (\n",
    "    filtered_df[filtered_df.duplicated(\"pattern_group\", keep=False)]\n",
    "    .groupby(\"pattern_group\")[\"fund_name\"]\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "expanded = grouped[\"fund_name\"].apply(pd.Series)\n",
    "expanded.columns = [f\"Match {i+1}\" for i in expanded.columns]\n",
    "expanded.to_csv(\"pattern_based_fund_name_matches.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"- 'funds_without_matches.csv'\")\n",
    "print(\"- 'pattern_based_fund_name_matches.csv'\")\n",
    "\n",
    "\n",
    "# Task 7 \n",
    "# Match Funds Only When Fund Manager IDs Align\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "with open(\"/Users/may/Downloads/Project _1/fundExc:Users:may:Downloads:Project _1:fundExclusions.jsonlusions.json\", \"r\") as f:\n",
    "    suffixes = json.load(f)\n",
    "\n",
    "suffixes = [re.sub(r'[^a-zA-Z0-9]', '', s.lower()) for s in suffixes]\n",
    "\n",
    "number_words = {\n",
    "    \"first\": \"1\", \"second\": \"2\", \"third\": \"3\", \"fourth\": \"4\", \"fifth\": \"5\", \"sixth\": \"6\", \"seventh\": \"7\",\n",
    "    \"eighth\": \"8\", \"ninth\": \"9\", \"tenth\": \"10\", \"eleventh\": \"11\", \"twelfth\": \"12\",\n",
    "    \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\", \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\",\n",
    "    \"eight\": \"8\", \"nine\": \"9\", \"ten\": \"10\", \"eleven\": \"11\", \"twelve\": \"12\"\n",
    "}\n",
    "\n",
    "roman_numerals = {\n",
    "    \"i\": \"1\", \"ii\": \"2\", \"iii\": \"3\", \"iv\": \"4\", \"v\": \"5\", \"vi\": \"6\", \"vii\": \"7\",\n",
    "    \"viii\": \"8\", \"ix\": \"9\", \"x\": \"10\", \"xi\": \"11\", \"xii\": \"12\"\n",
    "}\n",
    "\n",
    "df = pd.read_csv(\"/Users/may/Downloads/Project _1/funds16052025.csv\")\n",
    "\n",
    "if \"fund_manager_id\" not in df.columns:\n",
    "    raise ValueError(\"❌ Column 'fund_manager_id' not found in the CSV. Please check column names.\")\n",
    "\n",
    "def remove_legal_suffix(name):\n",
    "    original_name = str(name).strip()\n",
    "    normalized = re.sub(r'[^a-zA-Z0-9 ]', '', original_name.lower()).strip()\n",
    "\n",
    "    for suffix in suffixes:\n",
    "        if normalized.endswith(suffix):\n",
    "            pattern = rf'[\\s,\\.]*{re.escape(suffix)}[\\s,\\.]*$'\n",
    "            cleaned = re.sub(pattern, '', original_name, flags=re.IGNORECASE).strip()\n",
    "            return cleaned, suffix.upper()\n",
    "    return original_name, \"-\"\n",
    "\n",
    "def normalize_numbers(name):\n",
    "    words = name.split()\n",
    "    result = []\n",
    "    changes = []\n",
    "\n",
    "    for word in words:\n",
    "        word_lower = word.lower()\n",
    "        clean_word = re.sub(r'[^a-zA-Z]', '', word_lower)\n",
    "\n",
    "        if clean_word in number_words:\n",
    "            result.append(number_words[clean_word])\n",
    "            changes.append(f\"{word} → {number_words[clean_word]}\")\n",
    "        elif clean_word in roman_numerals:\n",
    "            result.append(roman_numerals[clean_word])\n",
    "            changes.append(f\"{word} → {roman_numerals[clean_word]}\")\n",
    "        else:\n",
    "            result.append(word)\n",
    "\n",
    "    return \" \".join(result), \", \".join(changes) if changes else \"-\"\n",
    "\n",
    "cleaned_names = []\n",
    "removed_suffixes = []\n",
    "number_changes = []\n",
    "\n",
    "for name in df[\"fund_name\"]:\n",
    "    no_suffix_name, suffix_removed = remove_legal_suffix(name)\n",
    "    final_name, number_change = normalize_numbers(no_suffix_name)\n",
    "\n",
    "    cleaned_names.append(final_name)\n",
    "    removed_suffixes.append(suffix_removed)\n",
    "    number_changes.append(number_change)\n",
    "\n",
    "df[\"Cleaned Fund Name\"] = cleaned_names\n",
    "df[\"Removed Suffix\"] = removed_suffixes\n",
    "df[\"Number Normalization\"] = number_changes\n",
    "\n",
    "df.to_csv(\"cleaned_fund_names_with_details.csv\", index=False)\n",
    "\n",
    "duplicates_df = df[df.duplicated(subset=[\"Cleaned Fund Name\"], keep=False)].copy()\n",
    "duplicates_df.to_csv(\"duplicate_cleaned_fund_names.csv\", index=False)\n",
    "\n",
    "match_groups = (\n",
    "    duplicates_df.groupby(\"Cleaned Fund Name\")\n",
    ")\n",
    "\n",
    "match_rows = []\n",
    "for _, group in match_groups:\n",
    "    fund_names = group[\"fund_name\"].tolist()\n",
    "    manager_ids = sorted(set(str(mid) for mid in group[\"fund_manager_id\"] if pd.notna(mid)))\n",
    "    row = fund_names + [\", \".join(manager_ids)]\n",
    "    match_rows.append(row)\n",
    "\n",
    "max_matches = max(len(row) - 1 for row in match_rows)\n",
    "columns = [f\"Match {i+1}\" for i in range(max_matches)] + [\"Fund Manager IDs\"]\n",
    "\n",
    "padded_rows = []\n",
    "for row in match_rows:\n",
    "    fund_names = row[:-1]\n",
    "    manager_ids = row[-1]\n",
    "    padded = fund_names + [\"\"] * (max_matches - len(fund_names)) + [manager_ids]\n",
    "    padded_rows.append(padded)\n",
    "\n",
    "final_df = pd.DataFrame(padded_rows, columns=columns)\n",
    "final_df.to_csv(\"exact_fund_name_matches.csv\", index=False)\n",
    "\n",
    "print(\"- exact_fund_name_matches.csv (with Fund Manager IDs)\")\n",
    "\n",
    "# Task 8 \n",
    "# (Task 4 Extension) : Exclude Previously Matched Funds and Manually Discover Additional Match Patterns\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from unidecode import unidecode\n",
    "\n",
    "df_original = pd.read_csv(\"/Users/may/Downloads/Project _1/funds16052025.csv\")\n",
    "\n",
    "with open(\"/Users/may/Downloads/Project _1/fundExc:Users:may:Downloads:Project _1:fundExclusions.jsonlusions.json\", \"r\") as f:\n",
    "    suffixes = json.load(f)\n",
    "suffixes = [re.sub(r'[^a-zA-Z0-9]', '', s.lower()) for s in suffixes]\n",
    "\n",
    "number_words = {\n",
    "    \"first\": \"1\", \"second\": \"2\", \"third\": \"3\", \"fourth\": \"4\", \"fifth\": \"5\", \"sixth\": \"6\",\n",
    "    \"seventh\": \"7\", \"eighth\": \"8\", \"ninth\": \"9\", \"tenth\": \"10\", \"eleventh\": \"11\", \"twelfth\": \"12\",\n",
    "    \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\", \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\",\n",
    "    \"eight\": \"8\", \"nine\": \"9\", \"ten\": \"10\", \"eleven\": \"11\", \"twelve\": \"12\"\n",
    "}\n",
    "roman_numerals = {\n",
    "    \"i\": \"1\", \"ii\": \"2\", \"iii\": \"3\", \"iv\": \"4\", \"v\": \"5\", \"vi\": \"6\", \"vii\": \"7\",\n",
    "    \"viii\": \"8\", \"ix\": \"9\", \"x\": \"10\", \"xi\": \"11\", \"xii\": \"12\"\n",
    "}\n",
    "equivalent_keywords = {\n",
    "    \"coinvestment\": \"coinvest\",\n",
    "    \"co-investment\": \"coinvest\",\n",
    "    \"co-invest\": \"coinvest\",\n",
    "    \"co- investment\": \"coinvest\",\n",
    "    \"co - investment\": \"coinvest\",\n",
    "    \"co – investment\": \"coinvest\",\n",
    "    \"co –investment\": \"coinvest\",\n",
    "    \"participations\": \"coinvest\",\n",
    "    \"program\": \"\",\n",
    "    \"fund\": \"\",\n",
    "    \"capital\": \"\",\n",
    "    \"ventures\": \"\",\n",
    "    \"venture\": \"\",\n",
    "    \"partner\": \"\",\n",
    "    \"equity\": \"\",\n",
    "    \"enterprise\": \"\",\n",
    "    \"opportunity\": \"\",\n",
    "    \"income\": \"\",\n",
    "    \"cooperatief\": \"\",\n",
    "    \"cooperatief ua\": \"\",\n",
    "    \"lp\": \"\",\n",
    "    \"ua\": \"\"\n",
    "}\n",
    "ignore_words = {\"india\", \"sarl\", \"sicar\"}\n",
    "\n",
    "def clean_name(name):\n",
    "    name = str(name).lower().strip()\n",
    "    name = unidecode(name)\n",
    "\n",
    "    cleaned = name\n",
    "    while True:\n",
    "        norm = re.sub(r'[^a-zA-Z0-9 ]', '', cleaned).strip()\n",
    "        matched = False\n",
    "        for suffix in suffixes:\n",
    "            if norm.endswith(suffix):\n",
    "                pattern = rf'[\\s,\\.]*{re.escape(suffix)}[\\s,\\.]*$'\n",
    "                cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE).strip()\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            break\n",
    "    name = cleaned\n",
    "\n",
    "    for phrase in [\"co- investment\", \"co - investment\", \"co – investment\", \"co –investment\"]:\n",
    "        name = name.replace(phrase, \"coinvestment\")\n",
    "\n",
    "    words = re.sub(r'[^a-zA-Z0-9 ]', '', name).split()\n",
    "    normalized = []\n",
    "\n",
    "    for word in words:\n",
    "        w = word.lower()\n",
    "        if w in number_words:\n",
    "            normalized.append(number_words[w])\n",
    "        elif w in roman_numerals:\n",
    "            normalized.append(roman_numerals[w])\n",
    "        elif w in ignore_words:\n",
    "            continue\n",
    "        elif w in equivalent_keywords:\n",
    "            if equivalent_keywords[w]:\n",
    "                normalized.append(equivalent_keywords[w])\n",
    "        else:\n",
    "            if w.isdigit():\n",
    "                normalized.append(str(int(w)))  \n",
    "            else:\n",
    "                normalized.append(w)\n",
    "\n",
    "    normalized = [w for w in normalized if w != \"1\"]\n",
    "\n",
    "    return \" \".join(sorted(normalized))\n",
    "\n",
    "df_original[\"cleaned_name\"] = df_original[\"fund_name\"].apply(clean_name)\n",
    "\n",
    "group_counts = df_original[\"cleaned_name\"].value_counts()\n",
    "duplicate_groups = set(group_counts[group_counts > 1].index)\n",
    "df_unmatched = df_original[~df_original[\"cleaned_name\"].isin(duplicate_groups)].copy()\n",
    "df_unmatched.drop(columns=[\"cleaned_name\"], inplace=True)\n",
    "\n",
    "df_unmatched.to_csv(\"funds_after_custom_group_removal.csv\", index=False)\n",
    "print(\"funds_after_custom_group_removal.csv\")\n",
    "\n",
    "\n",
    "   \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
